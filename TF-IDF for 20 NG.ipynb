{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb16d91",
   "metadata": {},
   "source": [
    "# Part 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9317f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "random.seed(20202200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7517e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe\n",
    "df = pd.read_json(\"newsgroups.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d6d8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181e5638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content         0\n",
       "target          0\n",
       "target_names    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheching if there is any missing value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7d3b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.sport.hockey            600\n",
       "soc.religion.christian      599\n",
       "rec.motorcycles             598\n",
       "rec.sport.baseball          597\n",
       "sci.crypt                   595\n",
       "sci.med                     594\n",
       "rec.autos                   594\n",
       "comp.windows.x              593\n",
       "sci.space                   593\n",
       "sci.electronics             591\n",
       "comp.os.ms-windows.misc     591\n",
       "comp.sys.ibm.pc.hardware    590\n",
       "misc.forsale                585\n",
       "comp.graphics               584\n",
       "comp.sys.mac.hardware       578\n",
       "talk.politics.mideast       564\n",
       "talk.politics.guns          546\n",
       "alt.atheism                 480\n",
       "talk.politics.misc          465\n",
       "talk.religion.misc          377\n",
       "Name: target_names, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target_names.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5058df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "# Take only the review_body column for unsupervised learning task\n",
    "\n",
    "data = df.loc[:, 'content'].tolist()\n",
    "print(type(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a523ff10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      " \n",
      "\n",
      "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
      "Subject: PB questions...\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Distribution: usa\n",
      "Lines: 36\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      " \n",
      "\n",
      "From: jgreen@amber (Joe Green)\n",
      "Subject: Re: Weitek P9000 ?\n",
      "Organization: Harris Computer Systems Division\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: amber.ssd.csd.harris.com\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "\n",
      "Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\n",
      "> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n",
      "> > Anyone know about the Weitek P9000 graphics chip?\n",
      "> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n",
      "> quadrilateral fill command that requires just the four points.\n",
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n",
      "--\n",
      "Joe Green\t\t\t\tHarris Corporation\n",
      "jgreen@csd.harris.com\t\t\tComputer Systems Division\n",
      "\"The only thing that really scares me is a person with no sense of humor.\"\n",
      "\t\t\t\t\t\t-- Jonathan Winters\n",
      " \n",
      "\n",
      "From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\n",
      "Subject: Re: Shuttle Launch Question\n",
      "Organization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\n",
      "Distribution: sci\n",
      "Lines: 23\n",
      "\n",
      "From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n",
      ">>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\n",
      ">>>\"Clear caution & warning memory.  Verify no unexpected\n",
      ">>>errors. ...\".  I am wondering what an \"expected error\" might\n",
      ">>>be.  Sorry if this is a really dumb question, but\n",
      "> \n",
      "> Parity errors in memory or previously known conditions that were waivered.\n",
      ">    \"Yes that is an error, but we already knew about it\"\n",
      "> I'd be curious as to what the real meaning of the quote is.\n",
      "> \n",
      "> tom\n",
      "\n",
      "\n",
      "My understanding is that the 'expected errors' are basically\n",
      "known bugs in the warning system software - things are checked\n",
      "that don't have the right values in yet because they aren't\n",
      "set till after launch, and suchlike. Rather than fix the code\n",
      "and possibly introduce new bugs, they just tell the crew\n",
      "'ok, if you see a warning no. 213 before liftoff, ignore it'.\n",
      "\n",
      " - Jonathan\n",
      "\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some of the reviews\n",
    "for _ in range(5):\n",
    "    print(data[_],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ac101",
   "metadata": {},
   "source": [
    "# Part 2: Tokenizing and Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7baf8a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use 179 stop-words from nltk library.\n"
     ]
    }
   ],
   "source": [
    "# Use nltk's English stopwords.\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "print(\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03392142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_and_stemming(text):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - string\n",
    "    OUTPUT\n",
    "    clean_tokens - a list of words\n",
    "    This function processes the input using the following steps :\n",
    "    1. Remove punctuation characters\n",
    "    2. Tokenize text into list\n",
    "    3. Stem, Normalize and Strip each word\n",
    "    4. Remove stop words\n",
    "    '''\n",
    "    # Remove punctuation characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Create a instance of stem class\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for word in tokens:\n",
    "        clean_tok = stemmer.stem(word).lower().strip()\n",
    "        if clean_tok not in stopwords:\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b40a689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab',\n",
       " 'cleveland',\n",
       " 'freenet',\n",
       " 'edu',\n",
       " 'sam',\n",
       " 'latonia',\n",
       " 'subject',\n",
       " 'need',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'western',\n",
       " 'digit',\n",
       " 'esdi',\n",
       " 'problem',\n",
       " 'organ',\n",
       " 'case',\n",
       " 'western',\n",
       " 'reserv',\n",
       " 'univers',\n",
       " 'cleveland',\n",
       " 'ohio',\n",
       " 'usa',\n",
       " 'line',\n",
       " 'nntp',\n",
       " 'post',\n",
       " 'host',\n",
       " 'slc',\n",
       " 'cwru',\n",
       " 'edu',\n",
       " 'western',\n",
       " 'digit',\n",
       " 'sam',\n",
       " 'gosh',\n",
       " 'think',\n",
       " 'instal',\n",
       " 'virus',\n",
       " 'call',\n",
       " 'ms',\n",
       " 'dos',\n",
       " 'copi',\n",
       " 'floppi',\n",
       " 'burn',\n",
       " 'love',\n",
       " 'window',\n",
       " 'crash']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization_and_stemming(data[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50d08e",
   "metadata": {},
   "source": [
    "# Part 3: c-TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "885af3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.utils.validation import FLOAT_DTYPES, check_is_fitted\n",
    "\n",
    "\n",
    "class CTFIDFVectorizer(TfidfTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CTFIDFVectorizer, self).__init__(*args, **kwargs)\n",
    "        self._idf_diag = None\n",
    "\n",
    "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
    "        \"\"\"Learn the idf vector (global term weights)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : sparse matrix of shape n_samples, n_features)\n",
    "            A matrix of term/token counts.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare input\n",
    "        X = check_array(X, accept_sparse=('csr', 'csc'))\n",
    "        if not sp.issparse(X):\n",
    "            X = sp.csr_matrix(X)\n",
    "        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64\n",
    "\n",
    "        # Calculate IDF scores\n",
    "        _, n_features = X.shape\n",
    "        df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
    "        avg_nr_samples = int(X.sum(axis=1).mean())\n",
    "        idf = np.log(avg_nr_samples / df)\n",
    "        self._idf_diag = sp.diags(idf, offsets=0,\n",
    "                                  shape=(n_features, n_features),\n",
    "                                  format='csr',\n",
    "                                  dtype=dtype)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: sp.csr_matrix, copy=True) -> sp.csr_matrix:\n",
    "        \"\"\"Transform a count-based matrix to c-TF-IDF\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : sparse matrix of (n_samples, n_features)\n",
    "            a matrix of term/token counts\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        vectors : sparse matrix of shape (n_samples, n_features)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare input\n",
    "        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)\n",
    "        if not sp.issparse(X):\n",
    "            X = sp.csr_matrix(X, dtype=np.float64)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # idf_ being a property, the automatic attributes detection\n",
    "        # does not work as usual and we need to specify the attribute\n",
    "        # name:\n",
    "        check_is_fitted(self, attributes=[\"idf_\"],\n",
    "                        msg='idf vector is not fitted')\n",
    "\n",
    "        # Check if expected nr features is found\n",
    "        expected_n_features = self._idf_diag.shape[0]\n",
    "        if n_features != expected_n_features:\n",
    "            raise ValueError(\"Input has n_features=%d while the model\"\n",
    "                             \" has been trained with n_features=%d\" % (\n",
    "                                 n_features, expected_n_features))\n",
    "\n",
    "        X = X * self._idf_diag\n",
    "\n",
    "        if self.norm:\n",
    "            X = normalize(X, axis=1, norm='l1', copy=False)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a37f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Create c-TF-IDF\n",
    "cv_matric = CountVectorizer().fit_transform(data)\n",
    "ctfidf_matrix = CTFIDFVectorizer().fit_transform(cv_matric, n_samples=len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "838a838c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1301018 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3326396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4605)\t-0.0034965034965034965\n",
      "  (0, 16574)\t0.006993006993006993\n",
      "  (0, 18299)\t0.006993006993006993\n",
      "  (0, 27436)\t-0.01048951048951049\n",
      "  (0, 28615)\t-0.013986013986013986\n",
      "  (0, 32311)\t-0.013986013986013986\n",
      "  (0, 34995)\t0.013986013986013986\n",
      "  (0, 35612)\t0.006993006993006993\n",
      "  (0, 35983)\t-0.01048951048951049\n",
      "  (0, 37433)\t-0.0034965034965034965\n",
      "  (0, 37565)\t-0.01048951048951049\n",
      "  (0, 37780)\t-0.017482517482517484\n",
      "  (0, 42876)\t-0.006993006993006993\n",
      "  (0, 45295)\t-0.0034965034965034965\n",
      "  (0, 48620)\t0.0034965034965034965\n",
      "  (0, 50527)\t-0.027972027972027972\n",
      "  (0, 51793)\t0.006993006993006993\n",
      "  (0, 56979)\t-0.04195804195804196\n",
      "  (0, 57308)\t0.01048951048951049\n",
      "  (0, 62221)\t-0.013986013986013986\n",
      "  (0, 64095)\t-0.006993006993006993\n",
      "  (0, 65798)\t-0.02097902097902098\n",
      "  (0, 66608)\t-0.017482517482517484\n",
      "  (0, 67156)\t-0.0034965034965034965\n",
      "  (0, 68532)\t-0.04195804195804196\n",
      "  :\t:\n",
      "  (11313, 76032)\t-0.02127659574468085\n",
      "  (11313, 76377)\t-0.0070921985815602835\n",
      "  (11313, 80638)\t-0.02127659574468085\n",
      "  (11313, 82355)\t0.0070921985815602835\n",
      "  (11313, 82480)\t0.0070921985815602835\n",
      "  (11313, 85354)\t-0.02127659574468085\n",
      "  (11313, 87620)\t-0.014184397163120567\n",
      "  (11313, 87626)\t-0.02127659574468085\n",
      "  (11313, 88363)\t-0.014184397163120567\n",
      "  (11313, 89362)\t-0.03546099290780142\n",
      "  (11313, 89860)\t-0.028368794326241134\n",
      "  (11313, 90252)\t-0.02127659574468085\n",
      "  (11313, 90379)\t-0.02127659574468085\n",
      "  (11313, 90946)\t-0.014184397163120567\n",
      "  (11313, 94291)\t0.0070921985815602835\n",
      "  (11313, 95162)\t-0.014184397163120567\n",
      "  (11313, 101950)\t0.0070921985815602835\n",
      "  (11313, 105818)\t-0.014184397163120567\n",
      "  (11313, 107339)\t0.0070921985815602835\n",
      "  (11313, 109661)\t0.0070921985815602835\n",
      "  (11313, 111322)\t-0.02127659574468085\n",
      "  (11313, 113435)\t0.02127659574468085\n",
      "  (11313, 113812)\t-0.0070921985815602835\n",
      "  (11313, 119714)\t-0.0070921985815602835\n",
      "  (11313, 124370)\t0.03546099290780142\n"
     ]
    }
   ],
   "source": [
    "print(ctfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d38731c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1301018 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(ctfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9afbddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4605)\t-0.0034965034965034965\n",
      "  (0, 16574)\t0.006993006993006993\n",
      "  (0, 18299)\t0.006993006993006993\n",
      "  (0, 27436)\t-0.01048951048951049\n",
      "  (0, 28615)\t-0.013986013986013986\n",
      "  (0, 32311)\t-0.013986013986013986\n",
      "  (0, 34995)\t0.013986013986013986\n",
      "  (0, 35612)\t0.006993006993006993\n",
      "  (0, 35983)\t-0.01048951048951049\n",
      "  (0, 37433)\t-0.0034965034965034965\n",
      "  (0, 37565)\t-0.01048951048951049\n",
      "  (0, 37780)\t-0.017482517482517484\n",
      "  (0, 42876)\t-0.006993006993006993\n",
      "  (0, 45295)\t-0.0034965034965034965\n",
      "  (0, 48620)\t0.0034965034965034965\n",
      "  (0, 50527)\t-0.027972027972027972\n",
      "  (0, 51793)\t0.006993006993006993\n",
      "  (0, 56979)\t-0.04195804195804196\n",
      "  (0, 57308)\t0.01048951048951049\n",
      "  (0, 62221)\t-0.013986013986013986\n",
      "  (0, 64095)\t-0.006993006993006993\n",
      "  (0, 65798)\t-0.02097902097902098\n",
      "  (0, 66608)\t-0.017482517482517484\n",
      "  (0, 67156)\t-0.0034965034965034965\n",
      "  (0, 68532)\t-0.04195804195804196\n",
      "  :\t:\n",
      "  (11313, 76032)\t-0.02127659574468085\n",
      "  (11313, 76377)\t-0.0070921985815602835\n",
      "  (11313, 80638)\t-0.02127659574468085\n",
      "  (11313, 82355)\t0.0070921985815602835\n",
      "  (11313, 82480)\t0.0070921985815602835\n",
      "  (11313, 85354)\t-0.02127659574468085\n",
      "  (11313, 87620)\t-0.014184397163120567\n",
      "  (11313, 87626)\t-0.02127659574468085\n",
      "  (11313, 88363)\t-0.014184397163120567\n",
      "  (11313, 89362)\t-0.03546099290780142\n",
      "  (11313, 89860)\t-0.028368794326241134\n",
      "  (11313, 90252)\t-0.02127659574468085\n",
      "  (11313, 90379)\t-0.02127659574468085\n",
      "  (11313, 90946)\t-0.014184397163120567\n",
      "  (11313, 94291)\t0.0070921985815602835\n",
      "  (11313, 95162)\t-0.014184397163120567\n",
      "  (11313, 101950)\t0.0070921985815602835\n",
      "  (11313, 105818)\t-0.014184397163120567\n",
      "  (11313, 107339)\t0.0070921985815602835\n",
      "  (11313, 109661)\t0.0070921985815602835\n",
      "  (11313, 111322)\t-0.02127659574468085\n",
      "  (11313, 113435)\t0.02127659574468085\n",
      "  (11313, 113812)\t-0.0070921985815602835\n",
      "  (11313, 119714)\t-0.0070921985815602835\n",
      "  (11313, 124370)\t0.03546099290780142\n"
     ]
    }
   ],
   "source": [
    "print(ctfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b222be",
   "metadata": {},
   "source": [
    "# Part 4: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9cf08645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model = KMeans(n_clusters=3)\n",
    "\n",
    "kmeans_model.fit(ctfidf_matrix) # Fit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9c224",
   "metadata": {},
   "source": [
    "#  Part 5: Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf67d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LDA for clustering\n",
    "LDA = LatentDirichletAllocation(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
