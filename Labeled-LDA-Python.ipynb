{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "075237be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# @Author: Jiahong Zhou\n",
    "# @Date: 2018-10-20\n",
    "# @Email: JoeZJiahong@gmail.com\n",
    "# implement of L-LDA Model(Labeled Latent Dirichlet Allocation Model)\n",
    "# References:\n",
    "#   i.      Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora, Daniel Ramage...\n",
    "#   ii.     Parameter estimation for text analysis, Gregor Heinrich.\n",
    "#   iii.    Latent Dirichlet Allocation, David M. Blei, Andrew Y. Ng...\n",
    "import numpy\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from concurrent import futures\n",
    "try:\n",
    "    import copy_reg\n",
    "except Exception:\n",
    "    import copyreg as copy_reg\n",
    "\n",
    "import types\n",
    "\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "class LldaModel:\n",
    "    \"\"\"\n",
    "    L-LDA(Labeled Latent Dirichlet Allocation Model)\n",
    "\n",
    "    @field K: the number of topics\n",
    "    @field alpha_vector: the prior distribution of theta_m\n",
    "                         str(\"50_div_K\"): means [K/50, K/50, ...],\n",
    "                                this value come from Parameter estimation for text analysis, Gregor Heinrich.\n",
    "                         int or float: means [alpha_vector, alpha_vector, ...]\n",
    "                         None: means [0.001, 0.001, ...]\n",
    "    @field eta_vector: the prior distribution of beta_k\n",
    "                       int or float: means [eta_vector, eta_vector, ...]\n",
    "                       None: means [0.001, 0.001, ...]\n",
    "    @field terms: a list of the all terms\n",
    "    @field vocabulary: a dict of <term, term_id>, vocabulary[terms[id]] == id\n",
    "    @field topics: a list of the all topics\n",
    "    @field topic_vocabulary: a dict of <topic, topic_id>, topic_vocabulary[topics[id]] == id\n",
    "    @field W: the corpus, a list of terms list,\n",
    "              W[m] is the document vector, W[m][n] is the id of the term\n",
    "    @field Z: the topic corpus, just same as W,\n",
    "              except Z[m][n] is the id of the topic of the term\n",
    "    @field M: the number of documents\n",
    "    @field T: the number of terms\n",
    "    @field WN: the number of all words in W\n",
    "    @field LN: the number of all original labels\n",
    "    @field iteration: the times of iteration\n",
    "    @field all_perplexities: a list of all perplexities (one training iteration one perplexity)\n",
    "    @field last_beta: the parameter `beta` of last training iteration\n",
    "    @field Lambda: a matrix, shape is M * K,\n",
    "                   Lambda[m][k] is 1 means topic k is a label of document m\n",
    "\n",
    "    # derivative fields\n",
    "    @field Doc2TopicCount: a matrix, shape is M * K,\n",
    "                           Doc2TopicCount[m][k] is the times of topic k sampled in document m\n",
    "    @field Topic2TermCount: a matrix, shape is K * T,\n",
    "                            Topic2TermCount[k][t] is the times of term t generated from topic k\n",
    "    @field Doc2TopicCountSum: a vector, shape is M, self.Doc2TopicCount.sum(axis=1)\n",
    "                              Doc2TopicCountSum[m] is the count of all topic,\n",
    "                              i.e., Doc2TopicCountSum[m] is the number of words in document m\n",
    "    @field alpha_vector_Lambda: a matrix, self.alpha_vector * self.Lambda\n",
    "    @field alpha_vector_Lambda_sum: a vector, self.alpha_vector_Lambda.sum(axis=1)\n",
    "    @field eta_vector_sum: float value, sum(self.eta_vector)\n",
    "    @field Topic2TermCountSum: a vector, self.Topic2TermCount.sum(axis=1)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha_vector=\"50_div_K\", eta_vector=None, labeled_documents=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param alpha_vector: the prior distribution of theta_m\n",
    "        :param eta_vector: the prior distribution of beta_k\n",
    "        :param labeled_documents: a iterable of tuple(doc, iterable of label), contains all doc and their labels\n",
    "        \"\"\"\n",
    "        self.alpha_vector = alpha_vector\n",
    "        self.eta_vector = eta_vector\n",
    "        self.terms = []\n",
    "        self.vocabulary = {}\n",
    "        self.topics = []\n",
    "        self.topic_vocabulary = {}\n",
    "        self.W = []\n",
    "        self.Z = []\n",
    "        self.K = 0\n",
    "        self.M = 0\n",
    "        self.T = 0\n",
    "        self.WN = 0\n",
    "        self.LN = 0\n",
    "        self.iteration = 0\n",
    "        self.all_perplexities = []\n",
    "        self.last_beta = None\n",
    "        self.Lambda = None\n",
    "\n",
    "        # derivative fields:\n",
    "        # the following fields could reduce operations in training and inference\n",
    "        # it is not necessary to save them to file, we can recover them by other fields\n",
    "\n",
    "        self.Doc2TopicCount = None\n",
    "        self.Topic2TermCount = None\n",
    "        # self.Doc2TopicCountSum = None\n",
    "        self.alpha_vector_Lambda = None\n",
    "        # self.alpha_vector_Lambda_sum = None\n",
    "        self.eta_vector_sum = 0.0\n",
    "        self.Topic2TermCountSum = None\n",
    "\n",
    "        if labeled_documents is not None:\n",
    "            self._load_labeled_documents(labeled_documents)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _initialize_derivative_fields(self):\n",
    "        \"\"\"\n",
    "        initialize derivative fields\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # TODO: Doc2TopicCount could be reduced to a smaller matrix,\n",
    "        # TODO: because some vector in Doc2TopicCount will always been 0\n",
    "        self.Doc2TopicCount = np.zeros((self.M, self.K), dtype=int)\n",
    "        self.Topic2TermCount = np.zeros((self.K, self.T), dtype=int)\n",
    "        for m in range(self.M):\n",
    "            # print self.Z[m]\n",
    "            for t, z in zip(self.W[m], self.Z[m]):\n",
    "                k = z\n",
    "                # print \"[m=%s, k=%s]\" % (m, k)\n",
    "                # print \"[k=%s, t=%s]\" % (k, t)\n",
    "                self.Doc2TopicCount[m, k] += 1\n",
    "                self.Topic2TermCount[k, t] += 1\n",
    "\n",
    "        # self.Doc2TopicCountSum = self.Doc2TopicCount.sum(axis=1)\n",
    "        self.alpha_vector_Lambda = self.alpha_vector * self.Lambda\n",
    "        # self.alpha_vector_Lambda_sum = self.alpha_vector_Lambda.sum(axis=1)\n",
    "        self.eta_vector_sum = sum(self.eta_vector)\n",
    "        self.Topic2TermCountSum = self.Topic2TermCount.sum(axis=1)\n",
    "\n",
    "    def _load_labeled_documents(self, labeled_documents):\n",
    "        \"\"\"\n",
    "        input labeled corpus, which contains all documents and their corresponding labels\n",
    "        :param labeled_documents: a iterable of tuple(doc, iterable of label), contains all doc and their labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # self.documents = []\n",
    "        all_labels = []\n",
    "        all_words = []\n",
    "        doc_corpus = []\n",
    "        labels_corpus = []\n",
    "        for document, labels in labeled_documents:\n",
    "            document = LldaModel._document_preprocess(document)\n",
    "            doc_words = document.split()\n",
    "            doc_corpus.append(doc_words)\n",
    "            if labels is None:\n",
    "                labels = []\n",
    "            labels.append(\"common_topic\")\n",
    "            labels_corpus.append(labels)\n",
    "            all_words.extend(doc_words)\n",
    "            all_labels.extend(labels)\n",
    "        self.terms = list(set(all_words))\n",
    "        self.vocabulary = {term: index for index, term in enumerate(self.terms)}\n",
    "        self.topics = list(set(all_labels))\n",
    "        self.topic_vocabulary = {topic: index for index, topic in enumerate(self.topics)}\n",
    "        self.K = len(self.topics)\n",
    "        self.T = len(self.terms)\n",
    "        self.W = [[self.vocabulary[term] for term in doc_words] for doc_words in doc_corpus]\n",
    "        self.M = len(self.W)\n",
    "        self.WN = len(all_words)\n",
    "        # we appended topic \"common_topic\" to each doc at the beginning\n",
    "        # so we need minus the number of \"common_topic\"\n",
    "        # LN is the number of original labels\n",
    "        self.LN = len(all_labels) - self.M\n",
    "\n",
    "        self.Lambda = np.zeros((self.M, self.K), dtype=float)\n",
    "        for m in range(self.M):\n",
    "            if len(labels_corpus[m]) == 1:\n",
    "                labels_corpus[m] = self.topics\n",
    "            for label in labels_corpus[m]:\n",
    "                k = self.topic_vocabulary[label]\n",
    "                self.Lambda[m, k] = 1.0\n",
    "\n",
    "        if self.alpha_vector is None:\n",
    "            self.alpha_vector = [0.001 for _ in range(self.K)]\n",
    "        elif type(self.alpha_vector) is str and self.alpha_vector == \"50_div_K\":\n",
    "            self.alpha_vector = [50.0/self.K for _ in range(self.K)]\n",
    "        elif type(self.alpha_vector) is float or type(self.alpha_vector) is int:\n",
    "            self.alpha_vector = [self.alpha_vector for _ in range(self.K)]\n",
    "        else:\n",
    "            message = \"error alpha_vector: %s\" % self.alpha_vector\n",
    "            raise Exception(message)\n",
    "\n",
    "        if self.eta_vector is None:\n",
    "            self.eta_vector = [0.001 for _ in range(self.T)]\n",
    "        elif type(self.eta_vector) is float or type(self.eta_vector) is int:\n",
    "            self.eta_vector = [self.eta_vector for _ in range(self.T)]\n",
    "        else:\n",
    "            message = \"error eta_vector: %s\" % self.eta_vector\n",
    "            raise Exception(message)\n",
    "\n",
    "        self.Z = []\n",
    "        for m in range(self.M):\n",
    "            # print \"self.Lambda[m]: \", self.Lambda[m]\n",
    "            numerator_vector = self.Lambda[m] * self.alpha_vector\n",
    "            p_vector = 1.0 * numerator_vector / sum(numerator_vector)\n",
    "            # print p_vector\n",
    "            # print \"p_vector: \", p_vector\n",
    "            # z_vector is a vector of a document,\n",
    "            # just like [2, 3, 6, 0], which means this doc have 4 word and them generated\n",
    "            # from the 2nd, 3rd, 6th, 0th topic, respectively\n",
    "            z_vector = [LldaModel._multinomial_sample(p_vector) for _ in range(len(self.W[m]))]\n",
    "            self.Z.append(z_vector)\n",
    "\n",
    "        self._initialize_derivative_fields()\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _multinomial_sample(p_vector, random_state=None):\n",
    "        \"\"\"\n",
    "        sample a number from multinomial distribution\n",
    "        :param p_vector: the probabilities\n",
    "        :return: a int value\n",
    "        \"\"\"\n",
    "        if random_state is not None:\n",
    "            return random_state.multinomial(1, p_vector).argmax()\n",
    "        return np.random.multinomial(1, p_vector).argmax()\n",
    "\n",
    "    def _gibbs_sample_training(self):\n",
    "        \"\"\"\n",
    "        sample a topic(k) for each word(t) of all documents, Generate a new matrix Z\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # TODO: the operations of addition and multiplication could be reduced, because some\n",
    "        self.last_beta = self.beta\n",
    "        count = 0\n",
    "        for m in range(self.M):\n",
    "\n",
    "            # doc_m_eta_vector = self.eta_vector\n",
    "            # doc_m_alpha_vector = self.alpha_vector * self.Lambda[m]\n",
    "            doc_m_alpha_vector = self.alpha_vector_Lambda[m]\n",
    "            # assert (doc_m_alpha_vector == self.alpha_vector_Lambda[m]).all()\n",
    "\n",
    "            # sum_doc_m_alpha_vector = sum(doc_m_alpha_vector)\n",
    "            # sum_doc_m_alpha_vector = self.alpha_vector_Lambda_sum[m]\n",
    "            # assert sum_doc_m_alpha_vector == self.alpha_vector_Lambda_sum[m]\n",
    "\n",
    "            for t, z, n in zip(self.W[m], self.Z[m], range(len(self.W[m]))):\n",
    "                k = z\n",
    "                self.Doc2TopicCount[m, k] -= 1\n",
    "                self.Topic2TermCount[k, t] -= 1\n",
    "                self.Topic2TermCountSum[k] -= 1\n",
    "\n",
    "                numerator_theta_vector = self.Doc2TopicCount[m] + doc_m_alpha_vector\n",
    "                # denominator_theta = sum(self.Doc2TopicCount[m]) + sum_doc_m_alpha_vector\n",
    "                # denominator_theta = self.Doc2TopicCountSum[m]-1 + sum_doc_m_alpha_vector\n",
    "                # assert sum(self.Doc2TopicCount[m]) == self.Doc2TopicCountSum[m]-1\n",
    "\n",
    "                numerator_beta_vector = self.Topic2TermCount[:, t] + self.eta_vector[t]\n",
    "                # denominator_beta = self.Topic2TermCount.sum(axis=1) + sum(self.eta_vector)\n",
    "                # denominator_beta = self.Topic2TermCount.sum(axis=1) + self.eta_vector_sum\n",
    "                denominator_beta = self.Topic2TermCountSum + self.eta_vector_sum\n",
    "                # assert (self.Topic2TermCount.sum(axis=1) == self.Topic2TermCountSum).all()\n",
    "                # assert sum(self.eta_vector) == self.eta_vector_sum\n",
    "\n",
    "                beta_vector = 1.0 * numerator_beta_vector / denominator_beta\n",
    "                # theta_vector = 1.0 * numerator_theta_vector / denominator_theta\n",
    "                # denominator_theta is independent with t and k, so denominator could be any value except 0\n",
    "                # will set denominator_theta as 1.0\n",
    "                theta_vector = numerator_theta_vector\n",
    "\n",
    "                p_vector = beta_vector * theta_vector\n",
    "                # print p_vector\n",
    "                \"\"\"\n",
    "                for some special document m (only have one word) p_vector may be zero here, sum(p_vector) will be zero too\n",
    "                1.0 * p_vector / sum(p_vector) will be [...nan...]\n",
    "                so we should avoid inputting the special document \n",
    "                \"\"\"\n",
    "                p_vector = 1.0 * p_vector / sum(p_vector)\n",
    "                # print p_vector\n",
    "                sample_z = LldaModel._multinomial_sample(p_vector)\n",
    "                self.Z[m][n] = sample_z\n",
    "\n",
    "                k = sample_z\n",
    "                self.Doc2TopicCount[m, k] += 1\n",
    "                self.Topic2TermCount[k, t] += 1\n",
    "                self.Topic2TermCountSum[k] += 1\n",
    "                count += 1\n",
    "        assert count == self.WN\n",
    "        print(\"gibbs sample count: \", self.WN)\n",
    "        self.iteration += 1\n",
    "        self.all_perplexities.append(self.perplexity())\n",
    "        pass\n",
    "\n",
    "    def _gibbs_sample_inference(self, term_vector, iteration=300, times=10):\n",
    "        \"\"\"\n",
    "        inference with gibbs sampling\n",
    "        :param term_vector: the term vector of document\n",
    "        :param iteration: the times of iteration until Markov chain converges\n",
    "        :param times: the number of samples of the target distribution\n",
    "                (one whole iteration(sample for all words) generates a sample, the )\n",
    "                #times = #samples,\n",
    "                after Markov chain converges, the next #times samples as the samples of the target distribution,\n",
    "                we drop the samples before the Markov chain converges,\n",
    "                the result is the average value of #times samples\n",
    "        :return: theta_new, a vector, theta_new[k] is the probability of doc(term_vector) to be generated from topic k\n",
    "                 theta_new, a theta_vector, the doc-topic distribution\n",
    "        \"\"\"\n",
    "        doc_topic_count = np.zeros(self.K, dtype=int)\n",
    "        accumulated_doc_topic_count = np.zeros(self.K, dtype=int)\n",
    "        p_vector = np.ones(self.K, dtype=int)\n",
    "        p_vector = p_vector * 1.0 / sum(p_vector)\n",
    "        z_vector = [LldaModel._multinomial_sample(p_vector) for _ in term_vector]\n",
    "        for n, t in enumerate(term_vector):\n",
    "            k = z_vector[n]\n",
    "            doc_topic_count[k] += 1\n",
    "            self.Topic2TermCount[k, t] += 1\n",
    "            self.Topic2TermCountSum[k] += 1\n",
    "\n",
    "        # sum_doc_topic_count = sum(doc_topic_count)\n",
    "        doc_m_alpha_vector = self.alpha_vector\n",
    "        # sum_doc_m_alpha_vector = sum(doc_m_alpha_vector)\n",
    "        for i in range(iteration+times):\n",
    "            for n, t in enumerate(term_vector):\n",
    "                k = z_vector[n]\n",
    "                doc_topic_count[k] -= 1\n",
    "                self.Topic2TermCount[k, t] -= 1\n",
    "                self.Topic2TermCountSum[k] -= 1\n",
    "\n",
    "                numerator_theta_vector = doc_topic_count + doc_m_alpha_vector\n",
    "                # denominator_theta = sum_doc_topic_count - 1 + sum_doc_m_alpha_vector\n",
    "\n",
    "                numerator_beta_vector = self.Topic2TermCount[:, t] + self.eta_vector[t]\n",
    "                # denominator_beta = self.Topic2TermCount.sum(axis=1) + sum(self.eta_vector)\n",
    "                denominator_beta = self.Topic2TermCountSum + self.eta_vector_sum\n",
    "\n",
    "                beta_vector = 1.0 * numerator_beta_vector / denominator_beta\n",
    "                # theta_vector = 1.0 numerator_theta_vector / denominator_theta\n",
    "                # denominator_theta is independent with t and k, so denominator could be any value except 0\n",
    "                # will set denominator_theta as 1.0\n",
    "                theta_vector = numerator_theta_vector\n",
    "\n",
    "                p_vector = beta_vector * theta_vector\n",
    "                # print p_vector\n",
    "                p_vector = 1.0 * p_vector / sum(p_vector)\n",
    "                # print p_vector\n",
    "                sample_z = LldaModel._multinomial_sample(p_vector)\n",
    "                z_vector[n] = sample_z\n",
    "\n",
    "                k = sample_z\n",
    "                doc_topic_count[k] += 1\n",
    "                self.Topic2TermCount[k, t] += 1\n",
    "                self.Topic2TermCountSum[k] += 1\n",
    "            if i >= iteration:\n",
    "                accumulated_doc_topic_count += doc_topic_count\n",
    "        # reset self.Topic2TermCount\n",
    "        for n, t in enumerate(term_vector):\n",
    "            k = z_vector[n]\n",
    "            self.Topic2TermCount[k, t] -= 1\n",
    "            self.Topic2TermCountSum[k] -= 1\n",
    "\n",
    "        numerator_theta_vector = accumulated_doc_topic_count/times + doc_m_alpha_vector\n",
    "        # denominator_theta = sum(doc_topic_count) + sum(doc_m_alpha_vector)\n",
    "        denominator_theta = sum(numerator_theta_vector)\n",
    "        theta_new = 1.0 * numerator_theta_vector / denominator_theta\n",
    "        return theta_new\n",
    "\n",
    "    # def _gibbs_sample_inference_multi_processors(self, term_vector, iteration=30):\n",
    "    #     \"\"\"\n",
    "    #     inference with gibbs sampling\n",
    "    #     :param term_vector: the term vector of document\n",
    "    #     :param iteration: the times of iteration\n",
    "    #     :return: theta_new, a vector, theta_new[k] is the probability of doc(term_vector) to be generated from topic k\n",
    "    #              theta_new, a theta_vector, the doc-topic distribution\n",
    "    #     \"\"\"\n",
    "    #     # print(\"gibbs sample inference iteration: %s\" % iteration)\n",
    "    #     # TODO: complete multi-processors code here\n",
    "    #     # we copy all the shared variables may be modified on runtime\n",
    "    #     random_state = np.random.RandomState()\n",
    "    #     topic2term_count = self.Topic2TermCount.copy()\n",
    "    #     topic2term_count_sum = self.Topic2TermCountSum.copy()\n",
    "    #\n",
    "    #     doc_topic_count = np.zeros(self.K, dtype=int)\n",
    "    #     p_vector = np.ones(self.K, dtype=int)\n",
    "    #     p_vector = p_vector * 1.0 / sum(p_vector)\n",
    "    #     z_vector = [LldaModel._multinomial_sample(p_vector, random_state=random_state) for _ in term_vector]\n",
    "    #     for n, t in enumerate(term_vector):\n",
    "    #         k = z_vector[n]\n",
    "    #         doc_topic_count[k] += 1\n",
    "    #         topic2term_count[k, t] += 1\n",
    "    #         topic2term_count_sum[k] += 1\n",
    "    #\n",
    "    #     # sum_doc_topic_count = sum(doc_topic_count)\n",
    "    #     doc_m_alpha_vector = self.alpha_vector\n",
    "    #     # sum_doc_m_alpha_vector = sum(doc_m_alpha_vector)\n",
    "    #     for i in range(iteration):\n",
    "    #         for n, t in enumerate(term_vector):\n",
    "    #             k = z_vector[n]\n",
    "    #             doc_topic_count[k] -= 1\n",
    "    #             topic2term_count[k, t] -= 1\n",
    "    #             topic2term_count_sum[k] -= 1\n",
    "    #\n",
    "    #             numerator_theta_vector = doc_topic_count + doc_m_alpha_vector\n",
    "    #             # denominator_theta = sum_doc_topic_count - 1 + sum_doc_m_alpha_vector\n",
    "    #\n",
    "    #             numerator_beta_vector = topic2term_count[:, t] + self.eta_vector[t]\n",
    "    #             # denominator_beta = self.Topic2TermCount.sum(axis=1) + sum(self.eta_vector)\n",
    "    #             denominator_beta = topic2term_count_sum + self.eta_vector_sum\n",
    "    #\n",
    "    #             beta_vector = 1.0 * numerator_beta_vector / denominator_beta\n",
    "    #             # theta_vector = 1.0 numerator_theta_vector / denominator_theta\n",
    "    #             # denominator_theta is independent with t and k, so denominator could be any value except 0\n",
    "    #             # will set denominator_theta as 1.0\n",
    "    #             theta_vector = numerator_theta_vector\n",
    "    #\n",
    "    #             p_vector = beta_vector * theta_vector\n",
    "    #             # print p_vector\n",
    "    #             p_vector = 1.0 * p_vector / sum(p_vector)\n",
    "    #             # print p_vector\n",
    "    #             sample_z = LldaModel._multinomial_sample(p_vector, random_state)\n",
    "    #             z_vector[n] = sample_z\n",
    "    #\n",
    "    #             k = sample_z\n",
    "    #             doc_topic_count[k] += 1\n",
    "    #             topic2term_count[k, t] += 1\n",
    "    #             topic2term_count_sum[k] += 1\n",
    "    #     # reset self.Topic2TermCount\n",
    "    #     # for n, t in enumerate(term_vector):\n",
    "    #     #     k = z_vector[n]\n",
    "    #     #     self.Topic2TermCount[k, t] -= 1\n",
    "    #     #     self.Topic2TermCountSum[k] -= 1\n",
    "    #\n",
    "    #     numerator_theta_vector = doc_topic_count + doc_m_alpha_vector\n",
    "    #     # denominator_theta = sum(doc_topic_count) + sum(doc_m_alpha_vector)\n",
    "    #     denominator_theta = sum(numerator_theta_vector)\n",
    "    #     theta_new = 1.0 * numerator_theta_vector / denominator_theta\n",
    "    #     return theta_new\n",
    "\n",
    "    def training(self, iteration=10, log=False):\n",
    "        \"\"\"\n",
    "        training this model with gibbs sampling\n",
    "        :param log: print perplexity after every gibbs sampling if True\n",
    "        :param iteration: the times of iteration\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            if log:\n",
    "                print(\"after iteration: %s, perplexity: %s\" % (self.iteration, self.perplexity()))\n",
    "            self._gibbs_sample_training()\n",
    "        pass\n",
    "\n",
    "    def inference(self, document, iteration=30, times=10):\n",
    "        # TODO: inference of a document\n",
    "        \"\"\"\n",
    "        inference for one document\n",
    "        :param document: some sentence like \"this is a method for inference\"\n",
    "        :param times: the number of samples of the target distribution\n",
    "                (one whole iteration(sample for all words) generates a sample, the )\n",
    "                #times = #samples,\n",
    "                after Markov chain converges, the next #times samples as the samples of the target distribution,\n",
    "                we drop the samples before the Markov chain converges,\n",
    "                the result is the average value of #times samples\n",
    "        :param iteration: the times of iteration until Markov chain converges\n",
    "        :return: theta_new, a vector, theta_new[k] is the probability of doc(term_vector) to be generated from topic k\n",
    "                 theta_new, a theta_vector, the doc-topic distribution\n",
    "        \"\"\"\n",
    "        document = LldaModel._document_preprocess(document)\n",
    "        doc_words = document.split()\n",
    "        term_vector = [self.vocabulary[word] for word in doc_words if word in self.vocabulary]\n",
    "        theta_new = self._gibbs_sample_inference(term_vector, iteration=iteration, times=times)\n",
    "        doc_topic_new = [(self.topics[k], probability) for k, probability in enumerate(theta_new)]\n",
    "        sorted_doc_topic_new = sorted(doc_topic_new,\n",
    "                                      key=lambda topic_probability: topic_probability[1],\n",
    "                                      reverse=True)\n",
    "        return sorted_doc_topic_new\n",
    "        pass\n",
    "\n",
    "    # def inference_multi_processors(self, document, iteration=30, times=8, max_workers=8):\n",
    "    #     # TODO: inference of a document with multi processors\n",
    "    #     \"\"\"\n",
    "    #     inference for one document\n",
    "    #     :param times: the times of gibbs sampling, the result is the average value of all times(gibbs sampling)\n",
    "    #     :param iteration: the times of iteration\n",
    "    #     :param document: some sentence like \"this is a method for inference\"\n",
    "    #     :param max_workers: the max number of processors(workers)\n",
    "    #     :return: theta_new, a vector, theta_new[k] is the probability of doc(term_vector) to be generated from topic k\n",
    "    #              theta_new, a theta_vector, the doc-topic distribution\n",
    "    #     \"\"\"\n",
    "    #\n",
    "    #     def _pickle_method(m):\n",
    "    #         if m.im_self is None:\n",
    "    #             return getattr, (m.im_class, m.im_func.func_name)\n",
    "    #         else:\n",
    "    #             return getattr, (m.im_self, m.im_func.func_name)\n",
    "    #     copy_reg.pickle(types.MethodType, _pickle_method)\n",
    "    #\n",
    "    #     words = document.split()\n",
    "    #     term_vector = [self.vocabulary[word] for word in words if word in self.vocabulary]\n",
    "    #     term_vectors = [term_vector for _ in range(times)]\n",
    "    #     iterations = [iteration for _ in range(times)]\n",
    "    #\n",
    "    #     with futures.ProcessPoolExecutor(max_workers) as executor:\n",
    "    #         # print \"executor.map\"\n",
    "    #         res = executor.map(self._gibbs_sample_inference_multi_processors, term_vectors, iterations)\n",
    "    #     theta_new_accumulation = np.zeros(self.K, float)\n",
    "    #     for theta_new in res:\n",
    "    #         theta_new_accumulation += theta_new\n",
    "    #     theta_new = 1.0 * theta_new_accumulation / times\n",
    "    #     # print \"avg: \\n\", theta_new\n",
    "    #     doc_topic_new = [(self.topics[k], probability) for k, probability in enumerate(theta_new)]\n",
    "    #     sorted_doc_topic_new = sorted(doc_topic_new,\n",
    "    #                                   key=lambda topic_probability: topic_probability[1],\n",
    "    #                                   reverse=True)\n",
    "    #     return sorted_doc_topic_new\n",
    "    #     pass\n",
    "\n",
    "    def beta_k(self, k):\n",
    "        \"\"\"\n",
    "        topic-term distribution\n",
    "        beta_k[t] is the probability of term t(word) to be generated from topic k\n",
    "        :return: a vector, shape is T\n",
    "        \"\"\"\n",
    "        numerator_vector = self.Topic2TermCount[k] + self.eta_vector\n",
    "        # denominator = sum(self.Topic2TermCount[k]) + sum(self.eta_vector)\n",
    "        denominator = sum(numerator_vector)\n",
    "        return 1.0 * numerator_vector / denominator\n",
    "\n",
    "    def theta_m(self, m):\n",
    "        \"\"\"\n",
    "        doc-topic distribution\n",
    "        theta_m[k] is the probability of doc m to be generated from topic k\n",
    "        :return: a vector, shape is K\n",
    "        \"\"\"\n",
    "        numerator_vector = self.Doc2TopicCount[m] + self.alpha_vector * self.Lambda[m]\n",
    "        # denominator = sum(self.Doc2TopicCount[m]) + sum(self.alpha_vector * self.Lambda[m])\n",
    "        denominator = sum(numerator_vector)\n",
    "        return 1.0 * numerator_vector / denominator\n",
    "\n",
    "    @property\n",
    "    def beta(self):\n",
    "        \"\"\"\n",
    "        This name \"beta\" comes from\n",
    "            \"Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora, Daniel Ramage...\"\n",
    "        topic-term distribution\n",
    "        beta[k, t] is the probability of term t(word) to be generated from topic k\n",
    "        :return: a matrix, shape is K * T\n",
    "        \"\"\"\n",
    "        numerator_matrix = self.Topic2TermCount + self.eta_vector\n",
    "        # column vector\n",
    "        # denominator_vector = self.Topic2TermCount.sum(axis=1).reshape(self.K, 1) + sum(self.eta_vector)\n",
    "        denominator_vector = numerator_matrix.sum(axis=1).reshape(self.K, 1)\n",
    "        return 1.0 * numerator_matrix / denominator_vector\n",
    "\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def theta(self):\n",
    "        \"\"\"\n",
    "        doc-topic distribution\n",
    "        theta[m, k] is the probability of doc m to be generated from topic k\n",
    "        :return: a matrix, shape is M * K\n",
    "        \"\"\"\n",
    "        numerator_matrix = self.Doc2TopicCount + self.alpha_vector * self.Lambda\n",
    "        denominator_vector = numerator_matrix.sum(axis=1).reshape(self.M, 1)\n",
    "        # column vector\n",
    "        return 1.0 * numerator_matrix / denominator_vector\n",
    "        pass\n",
    "\n",
    "    def log_perplexity(self, documents=None, iteration=30, times=10):\n",
    "        \"\"\"\n",
    "        log perplexity of LDA topic model, use the training data if documents is None\n",
    "        Reference:  Parameter estimation for text analysis, Gregor Heinrich.\n",
    "        :param: documents: test set\n",
    "        :return: a float value\n",
    "        \"\"\"\n",
    "        beta, theta, W, WN, log_likelihood = self.beta, None, None, None, 0\n",
    "        # theta is the doc-topic distribution matrix\n",
    "        # W is the list of term_vector, each term_vector represents a document\n",
    "        # WN is the number of all word in W\n",
    "        # difference test set means difference theta, W, WN\n",
    "\n",
    "        if not documents:\n",
    "            theta = self.theta\n",
    "            W = self.W\n",
    "            WN = self.WN\n",
    "        else:\n",
    "            # generate the term_vector of document\n",
    "            documents = [LldaModel._document_preprocess(document) for document in documents]\n",
    "            test_corpus = [document.split() for document in documents]\n",
    "            W = [[self.vocabulary[term] for term in doc_words if term in self.vocabulary] for doc_words in test_corpus]\n",
    "            WN = sum([len(term_vector) for term_vector in W])\n",
    "            theta = []\n",
    "            for term_vector in W:\n",
    "                # sample on term_vector until Markov chain converges\n",
    "                theta_new = self._gibbs_sample_inference(term_vector, iteration=iteration, times=times)\n",
    "                theta.append(theta_new)\n",
    "\n",
    "        # caculate the log_perplexity of current documents\n",
    "        for m, theta_m in enumerate(theta):\n",
    "            for t in W[m]:\n",
    "                likelihood_t = np.inner(theta_m, beta[:, t])\n",
    "                log_likelihood += -np.log(likelihood_t)\n",
    "        return 1.0 * log_likelihood / WN\n",
    "\n",
    "    def perplexity(self, documents=None, iteration=30, times=10):\n",
    "        \"\"\"\n",
    "        perplexity of LDA topic model, we use the training data if documents is None\n",
    "        Reference:  Parameter estimation for text analysis, Gregor Heinrich.\n",
    "        :param: documents: test set\n",
    "        :return: a float value, perplexity = exp{log_perplexity}\n",
    "        \"\"\"\n",
    "        return np.exp(self.log_perplexity(documents=documents, iteration=iteration, times=times))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\nLabeled-LDA Model:\\n\" \\\n",
    "               \"\\tK = %s\\n\" \\\n",
    "               \"\\tM = %s\\n\" \\\n",
    "               \"\\tT = %s\\n\" \\\n",
    "               \"\\tWN = %s\\n\" \\\n",
    "               \"\\tLN = %s\\n\" \\\n",
    "               \"\\talpha = %s\\n\" \\\n",
    "               \"\\teta = %s\\n\" \\\n",
    "               \"\\tperplexity = %s\\n\" \\\n",
    "               \"\\t\" % (self.K, self.M, self.T, self.WN, self.LN, self.alpha_vector[0], self.eta_vector[0],\n",
    "                       self.perplexity())\n",
    "        pass\n",
    "\n",
    "    class SaveModel:\n",
    "        def __init__(self, save_model_dict=None):\n",
    "            self.alpha_vector = []\n",
    "            self.eta_vector = []\n",
    "            self.terms = []\n",
    "            self.vocabulary = {}\n",
    "            self.topics = []\n",
    "            self.topic_vocabulary = {}\n",
    "            self.W = []\n",
    "            self.Z = []\n",
    "            self.K = 0\n",
    "            self.M = 0\n",
    "            self.T = 0\n",
    "            self.WN = 0\n",
    "            self.LN = 0\n",
    "            self.iteration = 0\n",
    "\n",
    "            # the following fields cannot be dumped into json file\n",
    "            # we need write them with np.save() and read them with np.load()\n",
    "            # self.Doc2TopicCount = None\n",
    "            # self.Topic2TermCount = None\n",
    "            self.Lambda = None\n",
    "\n",
    "            if save_model_dict is not None:\n",
    "                self.__dict__ = save_model_dict\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _document_preprocess(document):\n",
    "        \"\"\"\n",
    "        process document before inputting it into the model(both training, update and inference)\n",
    "        :param document: the target document\n",
    "        :return: the word we change\n",
    "        \"\"\"\n",
    "        document = document.lower()\n",
    "        return document\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_object_from_file(file_name):\n",
    "        \"\"\"\n",
    "        read an object from json file\n",
    "        :param file_name: json file name\n",
    "        :return: None if file doesn't exist or can not convert to an object by json, else return the object\n",
    "        \"\"\"\n",
    "        if os.path.exists(file_name) is False:\n",
    "            print (\"Error read path: [%s]\" % file_name)\n",
    "            return None\n",
    "        with open(file_name, 'r') as f:\n",
    "            try:\n",
    "                obj = json.load(f)\n",
    "            except Exception:\n",
    "                print (\"Error json: [%s]\" % f.read()[0:10])\n",
    "                return None\n",
    "        return obj\n",
    "\n",
    "    @staticmethod\n",
    "    def _write_object_to_file(file_name, target_object):\n",
    "        \"\"\"\n",
    "        write the object to file with json(if the file exists, this function will overwrite it)\n",
    "        :param file_name: the name of new file\n",
    "        :param target_object: the target object for writing\n",
    "        :return: True if success else False\n",
    "        \"\"\"\n",
    "        dirname = os.path.dirname(file_name)\n",
    "        LldaModel._find_and_create_dirs(dirname)\n",
    "        try:\n",
    "            with open(file_name, \"w\") as f:\n",
    "                json.dump(target_object, f, skipkeys=False, ensure_ascii=False, check_circular=True, allow_nan=True,\n",
    "                          cls=NpEncoder, indent=True, separators=None, default=None, sort_keys=False)\n",
    "        except Exception as e:\n",
    "            message = \"Write [%s...] to file [%s] error: json.dump error\" % (str(target_object)[0:10], file_name)\n",
    "            print (\"%s: %s\" % (e, message))\n",
    "            return False\n",
    "        else:\n",
    "            # print (\"Write %s\" % file_name)\n",
    "            return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_and_create_dirs(dir_name):\n",
    "        \"\"\"\n",
    "        find dir, create it if it doesn't exist\n",
    "        :param dir_name: the name of dir\n",
    "        :return: the name of dir\n",
    "        \"\"\"\n",
    "        if os.path.exists(dir_name) is False:\n",
    "            os.makedirs(dir_name)\n",
    "        return dir_name\n",
    "\n",
    "    def save_model_to_dir(self, dir_name, save_derivative_properties=False):\n",
    "        \"\"\"\n",
    "        save model to directory dir_name\n",
    "        :param save_derivative_properties: save derivative properties if True\n",
    "            some properties are not necessary save to disk, they could be derived from some basic properties,\n",
    "            we call they derivative properties.\n",
    "            to save derivative properties to disk:\n",
    "                it will reduce the time of loading model from disk (read properties directly but do not compute them)\n",
    "                but, meanwhile, it will take up more disk space\n",
    "        :param dir_name: the target directory name\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        save_model = LldaModel.SaveModel()\n",
    "        save_model.alpha_vector = self.alpha_vector\n",
    "        save_model.eta_vector = self.eta_vector\n",
    "        save_model.terms = self.terms\n",
    "        save_model.vocabulary = self.vocabulary\n",
    "        save_model.topics = self.topics\n",
    "        save_model.topic_vocabulary = self.topic_vocabulary\n",
    "        save_model.W = self.W\n",
    "        save_model.Z = self.Z\n",
    "        save_model.K = self.K\n",
    "        save_model.M = self.M\n",
    "        save_model.T = self.T\n",
    "        save_model.WN = self.WN\n",
    "        save_model.LN = self.LN\n",
    "        save_model.iteration = self.iteration\n",
    "\n",
    "        save_model_path = os.path.join(dir_name, \"llda_model.json\")\n",
    "        LldaModel._write_object_to_file(save_model_path, save_model.__dict__)\n",
    "\n",
    "        np.save(os.path.join(dir_name, \"Lambda.npy\"), self.Lambda)\n",
    "        # save derivative properties\n",
    "        if save_derivative_properties:\n",
    "            np.save(os.path.join(dir_name, \"Doc2TopicCount.npy\"), self.Doc2TopicCount)\n",
    "            np.save(os.path.join(dir_name, \"Topic2TermCount.npy\"), self.Topic2TermCount)\n",
    "            np.save(os.path.join(dir_name, \"alpha_vector_Lambda.npy\"), self.alpha_vector_Lambda)\n",
    "            np.save(os.path.join(dir_name, \"eta_vector_sum.npy\"), self.eta_vector_sum)\n",
    "            np.save(os.path.join(dir_name, \"Topic2TermCountSum.npy\"), self.Topic2TermCountSum)\n",
    "        pass\n",
    "\n",
    "    def load_model_from_dir(self, dir_name, load_derivative_properties=True):\n",
    "        \"\"\"\n",
    "        load model from directory dir_name\n",
    "        :param load_derivative_properties: load derivative properties from disk if True\n",
    "        :param dir_name: the target directory name\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        save_model_path = os.path.join(dir_name, \"llda_model.json\")\n",
    "        save_model_dict = LldaModel._read_object_from_file(save_model_path)\n",
    "        save_model = LldaModel.SaveModel(save_model_dict=save_model_dict)\n",
    "        self.alpha_vector = save_model.alpha_vector\n",
    "        self.eta_vector = save_model.eta_vector\n",
    "        self.terms = save_model.terms\n",
    "        self.vocabulary = save_model.vocabulary\n",
    "        self.topics = save_model.topics\n",
    "        self.topic_vocabulary = save_model.topic_vocabulary\n",
    "        self.W = save_model.W\n",
    "        self.Z = save_model.Z\n",
    "        self.K = save_model.K\n",
    "        self.M = save_model.M\n",
    "        self.T = save_model.T\n",
    "        self.WN = save_model.WN\n",
    "        self.LN = save_model.LN\n",
    "        self.iteration = save_model.iteration\n",
    "\n",
    "        self.Lambda = np.load(os.path.join(dir_name, \"Lambda.npy\"))\n",
    "\n",
    "        # load load_derivative properties\n",
    "        if load_derivative_properties:\n",
    "            try:\n",
    "                self.Doc2TopicCount = np.load(os.path.join(dir_name, \"Doc2TopicCount.npy\"))\n",
    "                self.Topic2TermCount = np.load(os.path.join(dir_name, \"Topic2TermCount.npy\"))\n",
    "                self.alpha_vector_Lambda = np.load(os.path.join(dir_name, \"alpha_vector_Lambda.npy\"))\n",
    "                self.eta_vector_sum = np.load(os.path.join(dir_name, \"eta_vector_sum.npy\"))\n",
    "                self.Topic2TermCountSum = np.load(os.path.join(dir_name, \"Topic2TermCountSum.npy\"))\n",
    "            except IOError or ValueError as e:\n",
    "                print(\"%s: load derivative properties fail, initialize them with basic properties\" % e)\n",
    "                self._initialize_derivative_fields()\n",
    "        else:\n",
    "            self._initialize_derivative_fields()\n",
    "        pass\n",
    "\n",
    "    def update(self, labeled_documents=None):\n",
    "        \"\"\"\n",
    "        update model with labeled documents, incremental update\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.all_perplexities = []\n",
    "        if labeled_documents is None:\n",
    "            pass\n",
    "\n",
    "        new_labels = []\n",
    "        new_words = []\n",
    "        new_doc_corpus = []\n",
    "        new_labels_corpus = []\n",
    "        for document, labels in labeled_documents:\n",
    "            document = LldaModel._document_preprocess(document)\n",
    "            doc_words = document.split()\n",
    "            new_doc_corpus.append(doc_words)\n",
    "            if labels is None:\n",
    "                labels = []\n",
    "            labels.append(\"common_topic\")\n",
    "            new_labels_corpus.append(labels)\n",
    "            new_words.extend(doc_words)\n",
    "            new_labels.extend(labels)\n",
    "        # self.terms = list(set(new_words))\n",
    "        new_terms = set(new_words) - set(self.terms)\n",
    "        self.terms.extend(new_terms)\n",
    "        self.vocabulary = {term: index for index, term in enumerate(self.terms)}\n",
    "\n",
    "        # self.topics = list(set(new_labels))\n",
    "        new_topics = set(new_labels) - set(self.topics)\n",
    "        self.topics.extend(new_topics)\n",
    "        self.topic_vocabulary = {topic: index for index, topic in enumerate(self.topics)}\n",
    "\n",
    "        old_K = self.K\n",
    "        old_T = self.T\n",
    "        self.K = len(self.topics)\n",
    "        self.T = len(self.terms)\n",
    "\n",
    "        # self.W = [[self.vocabulary[term] for term in doc_words] for doc_words in new_doc_corpus]\n",
    "        new_w_vectors = [[self.vocabulary[term] for term in doc_words] for doc_words in new_doc_corpus]\n",
    "        for new_w_vector in new_w_vectors:\n",
    "            self.W.append(new_w_vector)\n",
    "\n",
    "        old_M = self.M\n",
    "        old_WN = self.WN\n",
    "        self.M = len(self.W)\n",
    "        self.WN += len(new_words)\n",
    "        # we appended topic \"common_topic\" to each doc at the beginning\n",
    "        # so we need minus the number of \"common_topic\"\n",
    "        # LN is the number of original labels\n",
    "        old_LN = self.LN\n",
    "\n",
    "        self.LN += len(new_labels) + len(new_labels_corpus)\n",
    "\n",
    "        old_Lambda = self.Lambda\n",
    "        self.Lambda = np.zeros((self.M, self.K), dtype=float)\n",
    "        for m in range(self.M):\n",
    "            if m < old_M:\n",
    "                # if the old document has no topic, we also init it to all topics here\n",
    "                if sum(old_Lambda[m]) == old_K:\n",
    "                    # set all value of self.Lambda[m] to 1.0\n",
    "                    self.Lambda[m] += 1.0\n",
    "                continue\n",
    "            # print m, old_M\n",
    "            if len(new_labels_corpus[m-old_M]) == 1:\n",
    "                new_labels_corpus[m-old_M] = self.topics\n",
    "            for label in new_labels_corpus[m-old_M]:\n",
    "                k = self.topic_vocabulary[label]\n",
    "                self.Lambda[m, k] = 1.0\n",
    "\n",
    "        # TODO: the following 2 fields should be modified again if alpha_vector is not constant vector\n",
    "        self.alpha_vector = [self.alpha_vector[0] for _ in range(self.K)]\n",
    "        self.eta_vector = [self.eta_vector[0] for _ in range(self.T)]\n",
    "\n",
    "        # self.Z = []\n",
    "        for m in range(old_M, self.M):\n",
    "            # print \"self.Lambda[m]: \", self.Lambda[m]\n",
    "            numerator_vector = self.Lambda[m] * self.alpha_vector\n",
    "            p_vector = numerator_vector / sum(numerator_vector)\n",
    "            # print p_vector\n",
    "            # print \"p_vector: \", p_vector\n",
    "            # z_vector is a vector of a document,\n",
    "            # just like [2, 3, 6, 0], which means this doc have 4 word and them generated\n",
    "            # from the 2nd, 3rd, 6th, 0th topic, respectively\n",
    "            z_vector = [LldaModel._multinomial_sample(p_vector) for _ in range(len(self.W[m]))]\n",
    "            self.Z.append(z_vector)\n",
    "\n",
    "        self._initialize_derivative_fields()\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_matrix(origin=None, shape=None, padding_value=0):\n",
    "        \"\"\"\n",
    "        for quickly extend the matrices when update\n",
    "        extend origin matrix with shape, padding with padding_value\n",
    "        :type shape: the shape of new matrix\n",
    "        :param origin: np.ndarray, the original matrix\n",
    "        :return: np.ndarray, a matrix with new shape\n",
    "        \"\"\"\n",
    "        new_matrix = np.zeros(shape, dtype=origin.dtype)\n",
    "\n",
    "        for row in range(new_matrix.shape[0]):\n",
    "            for col in range(new_matrix.shape[1]):\n",
    "                if row < origin.shape[0] and col < origin.shape[0]:\n",
    "                    new_matrix[row, col] = origin[row, col]\n",
    "                else:\n",
    "                    new_matrix[row, col] = padding_value\n",
    "\n",
    "        return new_matrix\n",
    "        pass\n",
    "\n",
    "    def is_convergent(self, method=\"PPL\", delta=0.001):\n",
    "        \"\"\"\n",
    "        is this model convergent?\n",
    "        use the perplexities to determine whether the Markov chain converges\n",
    "        :param method: the method of determining whether the Markov chain converges\n",
    "                \"PPL\": use the perplexities of training data\n",
    "                \"beta\": use the parameter 'beta'\n",
    "        :param delta: if the changes are less than or equal to `delta`, means that the Markov chain converges\n",
    "        :return: True if model is convergent\n",
    "        \"\"\"\n",
    "        if method == \"PPL\":\n",
    "            if len(self.all_perplexities) < 10:\n",
    "                return False\n",
    "            perplexities = self.all_perplexities[-10:]\n",
    "            if max(perplexities) - min(perplexities) <= delta:\n",
    "                return True\n",
    "            return False\n",
    "        elif method == \"beta\":\n",
    "            if self.delta_beta <= delta:\n",
    "                return True\n",
    "            return False\n",
    "        else:\n",
    "            raise Exception(\"parameter 'method=\\\"%s\\\"' is illegal\" % method)\n",
    "\n",
    "    @property\n",
    "    def delta_beta(self):\n",
    "        \"\"\"\n",
    "        calculate the changes of the parameter `beta`\n",
    "        :return: the sum of changes of the parameter `beta`\n",
    "        \"\"\"\n",
    "        return np.sum(np.abs(self.beta - self.last_beta))\n",
    "\n",
    "    def top_terms_of_topic(self, topic, k, with_probabilities=True):\n",
    "        \"\"\"\n",
    "        get top-k terms of topic\n",
    "        :param with_probabilities: True means return the probabilities of a term generated by topic,\n",
    "                                   else return only terms\n",
    "        :param topic: str, the name of topic\n",
    "        :param k: int, the number of terms\n",
    "        :return: the top-k terms of topic\n",
    "        \"\"\"\n",
    "        if topic not in self.topic_vocabulary:\n",
    "            raise Exception(\"Cannot find topic \\\"%s\\\"\" % topic)\n",
    "        beta = self.beta_k(self.topic_vocabulary[topic])\n",
    "        terms = sorted(list(zip(self.terms, beta)), key=lambda x: x[1], reverse=True)\n",
    "        if with_probabilities:\n",
    "            return terms[:k]\n",
    "        return [term for term, p in terms[:k]]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b63a639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labeled-LDA Model:\n",
      "\tK = 6\n",
      "\tM = 5\n",
      "\tT = 12\n",
      "\tWN = 344\n",
      "\tLN = 7\n",
      "\talpha = 0.01\n",
      "\teta = 0.001\n",
      "\tperplexity = 4.000530017701293\n",
      "\t\n",
      "iteration 1 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 1, perplexity: 2.9240648794286925\n",
      "delta beta: 1.5348186444244956\n",
      "iteration 2 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 2, perplexity: 2.5473540965364205\n",
      "delta beta: 1.6111852390074337\n",
      "iteration 3 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 3, perplexity: 2.5090858131500418\n",
      "delta beta: 0.4911526619588684\n",
      "iteration 4 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 4, perplexity: 2.5075323228450657\n",
      "delta beta: 0.18700835654933665\n",
      "iteration 5 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 5, perplexity: 2.5059610646364563\n",
      "delta beta: 0.8555822434429641\n",
      "iteration 6 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 6, perplexity: 2.504373025966938\n",
      "delta beta: 0.35128892885264323\n",
      "iteration 7 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 7, perplexity: 2.5043730287509174\n",
      "delta beta: 0.1228022111960938\n",
      "iteration 8 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 8, perplexity: 2.504373029651785\n",
      "delta beta: 0.05675150356935145\n",
      "iteration 9 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 9, perplexity: 2.504373027735996\n",
      "delta beta: 0.10887543697806049\n",
      "iteration 10 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 10, perplexity: 2.5043730271813445\n",
      "delta beta: 0.02450030710998696\n",
      "iteration 11 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 11, perplexity: 2.5043730282585437\n",
      "delta beta: 0.050020008498151206\n",
      "iteration 12 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 12, perplexity: 2.5043730292147997\n",
      "delta beta: 0.05436378566565031\n",
      "iteration 13 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 13, perplexity: 2.504373002678004\n",
      "delta beta: 0.44777508764032925\n",
      "iteration 14 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 14, perplexity: 2.5043730072337493\n",
      "delta beta: 0.02531044179004009\n",
      "iteration 15 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 15, perplexity: 2.504373005066469\n",
      "delta beta: 0.012835497450090829\n",
      "iteration 16 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 16, perplexity: 2.504373002678004\n",
      "delta beta: 0.01247494433994926\n",
      "iteration 17 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 17, perplexity: 2.504372993797434\n",
      "delta beta: 0.03541999881978993\n",
      "iteration 18 sampling...\n",
      "gibbs sample count:  344\n",
      "after iteration: 18, perplexity: 2.504372993797434\n",
      "delta beta: 0.0\n",
      "before updating:  \n",
      "Labeled-LDA Model:\n",
      "\tK = 6\n",
      "\tM = 5\n",
      "\tT = 12\n",
      "\tWN = 344\n",
      "\tLN = 7\n",
      "\talpha = 0.01\n",
      "\teta = 0.001\n",
      "\tperplexity = 2.504372993797434\n",
      "\t\n",
      "after updating:  \n",
      "Labeled-LDA Model:\n",
      "\tK = 6\n",
      "\tM = 6\n",
      "\tT = 13\n",
      "\tWN = 353\n",
      "\tLN = 11\n",
      "\talpha = 0.01\n",
      "\teta = 0.001\n",
      "\tperplexity = 2.600480805873173\n",
      "\t\n",
      "iteration 19 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 19, perplexity: 2.538785944827251\n",
      "delta beta: 0.589045445829845\n",
      "iteration 20 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 20, perplexity: 2.5346937377651\n",
      "delta beta: 0.10138390089744057\n",
      "iteration 21 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 21, perplexity: 2.5426638055899513\n",
      "delta beta: 0.17231342960903406\n",
      "iteration 22 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 22, perplexity: 2.5346937561622522\n",
      "delta beta: 0.26253417015383773\n",
      "iteration 23 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 23, perplexity: 2.5393263532675947\n",
      "delta beta: 0.4344658354781018\n",
      "iteration 24 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 24, perplexity: 2.53041378853733\n",
      "delta beta: 0.2829856633380829\n",
      "iteration 25 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 25, perplexity: 2.530413788389505\n",
      "delta beta: 0.1249588632262941\n",
      "iteration 26 sampling...\n",
      "gibbs sample count:  353\n",
      "after iteration: 26, perplexity: 2.530413788389505\n",
      "delta beta: 0.0\n",
      "[('common_topic', 0.3325831984568654), ('positive', 0.22283440125933973), ('example', 0.22283440125933973), ('llda_model', 0.2217258275502738), ('negative', 1.1085737090659159e-05), ('test', 1.1085737090659159e-05)]\n",
      "perplexity on test data: 2.2553496305077414\n",
      "perplexity on training data: 2.530413788389505\n",
      "llda_model_new \n",
      "Labeled-LDA Model:\n",
      "\tK = 6\n",
      "\tM = 6\n",
      "\tT = 13\n",
      "\tWN = 353\n",
      "\tLN = 11\n",
      "\talpha = 0.01\n",
      "\teta = 0.001\n",
      "\tperplexity = 2.530413788389505\n",
      "\t\n",
      "llda_model \n",
      "Labeled-LDA Model:\n",
      "\tK = 6\n",
      "\tM = 6\n",
      "\tT = 13\n",
      "\tWN = 353\n",
      "\tLN = 11\n",
      "\talpha = 0.01\n",
      "\teta = 0.001\n",
      "\tperplexity = 2.530413788389505\n",
      "\t\n",
      "Top-5 terms of topic 'negative':  ['bad', 'down', 'downbad', 'modeltest', 'model']\n",
      "Doc-Topic Matrix: \n",
      " [[0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.        ]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.33333333 0.         0.66666667 0.        ]\n",
      " [1.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.99778516 0.         0.00110742 0.00110742 0.        ]]\n",
      "Topic-Term Matrix: \n",
      " [[1.63899497e-05 1.63899497e-05 5.08104830e-01 1.63899497e-05\n",
      "  1.47525937e-01 1.63899497e-05 1.63899497e-05 1.63899497e-05\n",
      "  1.63899497e-05 1.63899497e-05 3.44205333e-01 1.63899497e-05\n",
      "  1.63899497e-05]\n",
      " [1.24979691e-05 1.24979691e-05 1.24979691e-05 4.37441416e-01\n",
      "  1.24979691e-05 1.24979691e-05 1.24979691e-05 1.24979691e-05\n",
      "  4.37441416e-01 1.12494220e-01 1.24979691e-05 1.24979691e-05\n",
      "  1.25104670e-02]\n",
      " [3.33188951e-05 3.33188951e-05 3.33188951e-05 3.33188951e-05\n",
      "  3.33188951e-05 3.33188951e-05 9.99600173e-01 3.33188951e-05\n",
      "  3.33188951e-05 3.33188951e-05 3.33188951e-05 3.33188951e-05\n",
      "  3.33188951e-05]\n",
      " [2.43825129e-05 2.43825129e-05 2.43825129e-05 2.43825129e-05\n",
      "  2.43825129e-05 2.43825129e-05 2.43825129e-05 2.19466998e-01\n",
      "  7.80264794e-01 2.43825129e-05 2.43825129e-05 2.43825129e-05\n",
      "  2.43825129e-05]\n",
      " [1.66630563e-05 1.66630563e-05 1.66630563e-05 1.66630563e-05\n",
      "  1.66630563e-05 9.99800043e-01 1.66630563e-05 1.66630563e-05\n",
      "  1.66630563e-05 1.66630563e-05 1.66630563e-05 1.66630563e-05\n",
      "  1.66630563e-05]\n",
      " [1.11105625e-01 2.59230000e-01 1.23436979e-05 2.59230000e-01\n",
      "  1.23436979e-05 1.23436979e-05 1.23436979e-05 1.23436979e-05\n",
      "  1.23436979e-05 1.23436979e-05 1.23436979e-05 3.70323281e-01\n",
      "  1.23436979e-05]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import labeled_lda as llda\n",
    "\n",
    "# initialize data\n",
    "labeled_documents = [(\"example example example example example\"*10, [\"example\"]),\n",
    "                     (\"test llda model test llda model test llda model\"*10, [\"test\", \"llda_model\"]),\n",
    "                     (\"example test example test example test example test\"*10, [\"example\", \"test\"]),\n",
    "                     (\"good perfect good good perfect good good perfect good \"*10, [\"positive\"]),\n",
    "                     (\"bad bad down down bad bad down\"*10, [\"negative\"])]\n",
    "\n",
    "# new a Labeled LDA model\n",
    "# llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=\"50_div_K\", eta_vector=0.001)\n",
    "# llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=0.02, eta_vector=0.002)\n",
    "llda_model = llda.LldaModel(labeled_documents=labeled_documents, alpha_vector=0.01)\n",
    "print(llda_model)\n",
    "\n",
    "# training\n",
    "# llda_model.training(iteration=10, log=True)\n",
    "while True:\n",
    "    print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
    "    llda_model.training(1)\n",
    "    print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
    "    print(\"delta beta: %s\" % llda_model.delta_beta)\n",
    "    if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
    "        break\n",
    "\n",
    "# update\n",
    "print(\"before updating: \", llda_model)\n",
    "update_labeled_documents = [(\"new example test example test example test example test\", [\"example\", \"test\"])]\n",
    "llda_model.update(labeled_documents=update_labeled_documents)\n",
    "print(\"after updating: \", llda_model)\n",
    "\n",
    "# train again\n",
    "# llda_model.training(iteration=10, log=True)\n",
    "while True:\n",
    "    print(\"iteration %s sampling...\" % (llda_model.iteration + 1))\n",
    "    llda_model.training(1)\n",
    "    print(\"after iteration: %s, perplexity: %s\" % (llda_model.iteration, llda_model.perplexity()))\n",
    "    print(\"delta beta: %s\" % llda_model.delta_beta)\n",
    "    if llda_model.is_convergent(method=\"beta\", delta=0.01):\n",
    "        break\n",
    "\n",
    "# inference\n",
    "# note: the result topics may be different for difference training, because gibbs sampling is a random algorithm\n",
    "document = \"example llda model example example good perfect good perfect good perfect\" * 100\n",
    "\n",
    "topics = llda_model.inference(document=document, iteration=100, times=10)\n",
    "print(topics)\n",
    "\n",
    "# perplexity\n",
    "# calculate perplexity on test data\n",
    "perplexity = llda_model.perplexity(documents=[\"example example example example example\",\n",
    "                                              \"test llda model test llda model test llda model\",\n",
    "                                              \"example test example test example test example test\",\n",
    "                                              \"good perfect good good perfect good good perfect good\",\n",
    "                                              \"bad bad down down bad bad down\"],\n",
    "                                   iteration=30,\n",
    "                                   times=10)\n",
    "print(\"perplexity on test data: %s\" % perplexity)\n",
    "# calculate perplexity on training data\n",
    "print(\"perplexity on training data: %s\" % llda_model.perplexity())\n",
    "\n",
    "# save to disk\n",
    "save_model_dir = \"../data/model\"\n",
    "# llda_model.save_model_to_dir(save_model_dir, save_derivative_properties=True)\n",
    "llda_model.save_model_to_dir(save_model_dir)\n",
    "\n",
    "# load from disk\n",
    "llda_model_new = llda.LldaModel()\n",
    "llda_model_new.load_model_from_dir(save_model_dir, load_derivative_properties=False)\n",
    "print(\"llda_model_new\", llda_model_new)\n",
    "print(\"llda_model\", llda_model)\n",
    "print(\"Top-5 terms of topic 'negative': \", llda_model.top_terms_of_topic(\"negative\", 5, False))\n",
    "print(\"Doc-Topic Matrix: \\n\", llda_model.theta)\n",
    "print(\"Topic-Term Matrix: \\n\", llda_model.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a40b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
