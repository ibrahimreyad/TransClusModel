{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e3fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import codecs\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9849f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (word segmentation, remove stop words, assign a number to each word, the document is represented by a list of word numbers)\n",
    "def preprocessing():\n",
    "    # Read stop word file\n",
    "    file = codecs.open('stopwords.dic','r','utf-8')\n",
    "    stopwords = [line.strip() for line in file] \n",
    "    file.close()\n",
    "    \n",
    "    # read dataset\n",
    "    ##file = codecs.open('dataset.txt','r','utf-8')\n",
    "    file = codecs.open('newsgroups.json','r','utf-8')\n",
    "    documents = [document.strip() for document in file] \n",
    "    file.close()\n",
    "    \n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    docs = []\n",
    "    currentDocument = []\n",
    "    currentWordId = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        #  Participle\n",
    "        segList = jieba.cut(document)\n",
    "        for word in segList: \n",
    "            word = word.lower().strip()\n",
    "            # Word length is greater than 1 and does not contain numbers and is not a stop word\n",
    "            if len(word) > 1 and not re.search('[0-9]', word) and word not in stopwords:\n",
    "                if word in word2id:\n",
    "                    currentDocument.append(word2id[word])\n",
    "                else:\n",
    "                    currentDocument.append(currentWordId)\n",
    "                    word2id[word] = currentWordId\n",
    "                    id2word[currentWordId] = word\n",
    "                    currentWordId += 1\n",
    "        docs.append(currentDocument);\n",
    "        currentDocument = []\n",
    "    return docs, word2id, id2word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4743c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization, sampling according to the multinomial distribution with equal probability of each topic, which is equivalent to taking random numbers and updating the relevant counts of the sampled topics\n",
    "def randomInitialize():\n",
    "\tfor d, doc in enumerate(docs):\n",
    "\t\tzCurrentDoc = []\n",
    "\t\tfor w in doc:\n",
    "\t\t\tpz = np.divide(np.multiply(ndz[d, :], nzw[:, w]), nz)\n",
    "\t\t\tz = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "\t\t\tzCurrentDoc.append(z)\n",
    "\t\t\tndz[d, z] += 1\n",
    "\t\t\tnzw[z, w] += 1\n",
    "\t\t\tnz[z] += 1\n",
    "\t\tZ.append(zCurrentDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "387bca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gibbs sampling\n",
    "def gibbsSampling():\n",
    "\t# resample topic for each word in each document\n",
    "\tfor d, doc in enumerate(docs):\n",
    "\t\tfor index, w in enumerate(doc):\n",
    "\t\t\tz = Z[d][index]\n",
    "\t\t\t# Subtract 1 from the original topic related count of the current word in the current document\n",
    "\t\t\tndz[d, z] -= 1\n",
    "\t\t\tnzw[z, w] -= 1\n",
    "\t\t\tnz[z] -= 1\n",
    "\t\t\t# Recalculate the probability that the current word in the current document belongs to each topic\n",
    "\t\t\tpz = np.divide(np.multiply(ndz[d, :], nzw[:, w]), nz)\n",
    "\t\t\t# Sampling according to the calculated distribution\n",
    "\t\t\tz = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "\t\t\tZ[d][index] = z \n",
    "\t\t\t# Add 1 to the newly sampled topic-related count of the current word in the current document\n",
    "\t\t\tndz[d, z] += 1\n",
    "\t\t\tnzw[z, w] += 1\n",
    "\t\t\tnz[z] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ae83b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity():\n",
    "\tnd = np.sum(ndz, 1)\n",
    "\tn = 0\n",
    "\tll = 0.0\n",
    "\tfor d, doc in enumerate(docs):\n",
    "\t\tfor w in doc:\n",
    "\t\t\tll = ll + np.log(((nzw[:, w] / nz) * (ndz[d, :] / nd[d])).sum())\n",
    "\t\t\tn = n + 1\n",
    "\treturn np.exp(ll/(-n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230ffd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:35:17 Iteration:  0  Completed  Perplexity:  9714.22247673278\n",
      "00:36:14 Iteration:  1  Completed  Perplexity:  9712.289454574417\n",
      "00:37:07 Iteration:  2  Completed  Perplexity:  9710.751329046383\n",
      "00:38:10 Iteration:  3  Completed  Perplexity:  9709.177530021467\n",
      "00:39:09 Iteration:  4  Completed  Perplexity:  9707.924082904106\n"
     ]
    }
   ],
   "source": [
    "alpha = 5\n",
    "beta = 0.1\t\n",
    "iterationNum = 5\n",
    "Z = []\n",
    "K = 10\n",
    "docs, word2id, id2word = preprocessing()\n",
    "N = len(docs)\n",
    "M = len(word2id)\n",
    "ndz = np.zeros([N, K]) + alpha\n",
    "nzw = np.zeros([K, M]) + beta\n",
    "nz = np.zeros([K]) + M * beta\n",
    "randomInitialize()\n",
    "for i in range(0, iterationNum):\n",
    "\tgibbsSampling()\n",
    "\tprint(time.strftime('%X'), \"Iteration: \", i, \" Completed\", \" Perplexity: \", perplexity())\n",
    " \n",
    "topicwords = []\n",
    "maxTopicWordsNum = 10\n",
    "for z in range(0, K):\n",
    "\tids = nzw[z, :].argsort()\n",
    "\ttopicword = []\n",
    "\tfor j in ids:\n",
    "\t\ttopicword.insert(0, id2word[j])\n",
    "\ttopicwords.append(topicword[0 : min(10, len(topicword))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9bc735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nm', 'ni', 've', 'mail', 'nthat', 'time', 'usa', 'public', 'david', 'nto'],\n",
       " ['ibm',\n",
       "  'memory',\n",
       "  'won',\n",
       "  'top',\n",
       "  'time',\n",
       "  'tv',\n",
       "  'freenet',\n",
       "  'america',\n",
       "  'input',\n",
       "  'designed'],\n",
       " ['nlines',\n",
       "  'writes',\n",
       "  'article',\n",
       "  'people',\n",
       "  'don',\n",
       "  'university',\n",
       "  'host',\n",
       "  'posting',\n",
       "  'windows',\n",
       "  'cs'],\n",
       " ['nhouston',\n",
       "  'gloves',\n",
       "  'ireland',\n",
       "  'modifications',\n",
       "  'td',\n",
       "  'asserts',\n",
       "  'paslawski',\n",
       "  'beware',\n",
       "  'resurection',\n",
       "  '+-----+'],\n",
       " ['computer',\n",
       "  'bit',\n",
       "  'mit',\n",
       "  'write',\n",
       "  'nhave',\n",
       "  'nby',\n",
       "  'atheists',\n",
       "  'cut',\n",
       "  'happened',\n",
       "  'acs'],\n",
       " ['nof',\n",
       "  'god',\n",
       "  'ndistribution',\n",
       "  'power',\n",
       "  'info',\n",
       "  'files',\n",
       "  'list',\n",
       "  'time',\n",
       "  'answer',\n",
       "  'nnntp'],\n",
       " ['christ',\n",
       "  'coding',\n",
       "  'seed',\n",
       "  'nt',\n",
       "  'humanity',\n",
       "  'wrist',\n",
       "  'sspx',\n",
       "  'tif',\n",
       "  'computer',\n",
       "  'frej'],\n",
       " ['lifetime',\n",
       "  'questor',\n",
       "  'nicholas',\n",
       "  'plays',\n",
       "  'nrespect',\n",
       "  'sepinwal',\n",
       "  'method',\n",
       "  'crude',\n",
       "  'snefru',\n",
       "  'pollution'],\n",
       " ['ax',\n",
       "  'nsubject',\n",
       "  'norganization',\n",
       "  'nthe',\n",
       "  'nin',\n",
       "  'nx',\n",
       "  'nmax',\n",
       "  'nnntp',\n",
       "  'nand',\n",
       "  'comp'],\n",
       " ['national',\n",
       "  'drivers',\n",
       "  'phone',\n",
       "  'fire',\n",
       "  'thinking',\n",
       "  'driver',\n",
       "  'knowledge',\n",
       "  'electronic',\n",
       "  'institute',\n",
       "  'book']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528529c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
