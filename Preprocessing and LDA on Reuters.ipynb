{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fbd5ec",
   "metadata": {},
   "source": [
    "# Step 1: Load the Reuters dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3397c20",
   "metadata": {},
   "source": [
    "### You can download the Reuters dataset from the NLTK library using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd95db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('reuters')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c79ec",
   "metadata": {},
   "source": [
    "### Once you have downloaded the dataset, you can load it using the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b18f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "documents = reuters.fileids()\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ea47c",
   "metadata": {},
   "source": [
    "# Step 2: Preprocess the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d4fda",
   "metadata": {},
   "source": [
    "### You will need to preprocess the dataset by tokenizing the text, removing stop words, and stemming the words.\n",
    "### Here is some example code to preprocess the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499867f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "corpus = [preprocess(reuters.raw(document_id)) for document_id in documents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb016c",
   "metadata": {},
   "source": [
    "# Step 3: Find bigrams in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0ab98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_documents(corpus)\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stop_words)\n",
    "finder.apply_freq_filter(5)\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23c468",
   "metadata": {},
   "source": [
    "###  Add bigrams to the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40bc3df",
   "metadata": {},
   "source": [
    "# Step 3: Create a dictionary and bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bb22f",
   "metadata": {},
   "source": [
    "### You will need to create a dictionary and a bag of words from the preprocessed corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1eb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "dictionary = Dictionary(corpus)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe066507",
   "metadata": {},
   "source": [
    "# Step 4: Train the LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4740738",
   "metadata": {},
   "source": [
    "### You can use the Gensim library to train the LDA model on the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b764a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "num_topics = 10\n",
    "lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd535ac",
   "metadata": {},
   "source": [
    "### We can print out the top words in each topic to get an idea of what each topic is about:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "165b9c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.048*\"said\" + 0.030*\"lt\" + 0.026*\"compani\" + 0.024*\"share\" + 0.021*\"dlrs\" + 0.015*\"mln\" + 0.013*\"inc\" + 0.012*\"corp\" + 0.012*\"pct\" + 0.011*\"offer\"')\n",
      "(1, '0.034*\"said\" + 0.016*\"gold\" + 0.014*\"produc\" + 0.014*\"mine\" + 0.014*\"export\" + 0.013*\"coffe\" + 0.012*\"price\" + 0.011*\"brazil\" + 0.011*\"sugar\" + 0.011*\"china\"')\n",
      "(2, '0.137*\"vs\" + 0.115*\"mln\" + 0.068*\"net\" + 0.062*\"cts\" + 0.057*\"loss\" + 0.046*\"dlrs\" + 0.040*\"shr\" + 0.030*\"profit\" + 0.025*\"qtr\" + 0.024*\"lt\"')\n",
      "(3, '0.026*\"ec\" + 0.023*\"franc\" + 0.021*\"said\" + 0.016*\"european\" + 0.013*\"french\" + 0.011*\"credit\" + 0.010*\"sugar\" + 0.010*\"communiti\" + 0.009*\"would\" + 0.009*\"commiss\"')\n",
      "(4, '0.085*\"billion\" + 0.070*\"bank\" + 0.052*\"mln\" + 0.039*\"dlrs\" + 0.028*\"pct\" + 0.027*\"stg\" + 0.019*\"reserv\" + 0.019*\"loan\" + 0.016*\"money\" + 0.016*\"said\"')\n",
      "(5, '0.064*\"cts\" + 0.037*\"lt\" + 0.037*\"april\" + 0.036*\"dividend\" + 0.033*\"record\" + 0.022*\"pay\" + 0.022*\"div\" + 0.021*\"quarter\" + 0.021*\"prior\" + 0.019*\"vs\"')\n",
      "(6, '0.036*\"said\" + 0.016*\"trade\" + 0.016*\"market\" + 0.012*\"rate\" + 0.012*\"dollar\" + 0.012*\"japan\" + 0.011*\"bank\" + 0.011*\"would\" + 0.008*\"exchang\" + 0.007*\"currenc\"')\n",
      "(7, '0.045*\"pct\" + 0.037*\"said\" + 0.033*\"year\" + 0.012*\"rise\" + 0.011*\"expect\" + 0.011*\"increas\" + 0.011*\"price\" + 0.010*\"last\" + 0.010*\"januari\" + 0.010*\"februari\"')\n",
      "(8, '0.058*\"tonn\" + 0.040*\"mln\" + 0.023*\"said\" + 0.022*\"wheat\" + 0.021*\"export\" + 0.018*\"grain\" + 0.018*\"corn\" + 0.013*\"depart\" + 0.013*\"usda\" + 0.012*\"soybean\"')\n",
      "(9, '0.033*\"said\" + 0.031*\"oil\" + 0.013*\"would\" + 0.009*\"trade\" + 0.009*\"price\" + 0.008*\"barrel\" + 0.007*\"crude\" + 0.007*\"product\" + 0.007*\"state\" + 0.006*\"gas\"')\n"
     ]
    }
   ],
   "source": [
    "# Print topics and top words in each topic\n",
    "for topic in lda_model.show_topics(num_topics=num_topics):\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ddbb7f",
   "metadata": {},
   "source": [
    "### To visualize the topics, we can use the pyLDAvis library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e29fe",
   "metadata": {},
   "source": [
    "### This will display an interactive visualization of the topics, where each bubble represents a topic and the size of the bubble corresponds to the prevalence of the topic in the corpus. The closer two bubbles are to each other, the more similar their topics are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e221f",
   "metadata": {},
   "source": [
    "### We can also visualize the distribution of topics in the corpus using a histogram:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687886fb",
   "metadata": {},
   "source": [
    "# Step 5: Calculate evaluation metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c8d5e",
   "metadata": {},
   "source": [
    "### To calculate the evaluation metrics, you will need to assign a topic to each document.\n",
    "### Here is some example code to assign topics to each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "760605ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics = []\n",
    "for document_bow in bow_corpus:\n",
    "    document_topic = max(lda_model[document_bow], key=lambda x: x[1])[0]\n",
    "    document_topics.append(document_topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106e371",
   "metadata": {},
   "source": [
    "### Once you have assigned topics to each document, you can calculate the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576bf9b",
   "metadata": {},
   "source": [
    "### Note that the PMI metric requires a bit of additional preprocessing to calculate bigram associations in the text.\n",
    "### That's it! You have now trained an LDA model on the Reuters dataset and calculated evaluation metrics for the topic assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a86697",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609226a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc08932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feba8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e7ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
